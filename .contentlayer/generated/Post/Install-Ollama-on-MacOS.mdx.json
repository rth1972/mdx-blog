{
  "title": "Install Ollama and llama3 on MacOS",
  "date": "2024-05-24T00:00:00.000Z",
  "tags": [
    "Ollama",
    "guide"
  ],
  "body": {
    "raw": "\n# Install Ollama on MacOS\n\n### what is Ollama?\n\nOllama is an open-source tool that allows you to run and customize large language models locally on your own machine.\n\nHere are some key points about Ollama:\n\n* Local Integration: Ollama enables you to work with language models directly on your machine, avoiding the need for external services or APIs.\n* Model Support: It supports various language models, including Llama 3, Mistral, Gemma, and more. You can choose the model that best suits your needs.\n* CLI and REST API: Ollama provides a command-line interface (CLI) for running and customizing models. Additionally, it offers a REST API for programmatic access.\n* Customization: You can create and customize models using Ollama. For example, you can adjust parameters like temperature to control creativity and coherence.\n* Model Library: Ollama includes a library of pre-trained models that you can download and use. These models cover a range of sizes and capabilities.\n* RAM Requirements: Keep in mind that running larger models (e.g., 7B models) may require a minimum of 8 GB of available RAM.\n\n\n### In this article, I will show you how to get started with Ollama on a Mac.\n\n>* These instructions were written for and tested on a Mac (M1, 8GB).\n>* You will have much better success on a Mac that uses Apple Silicon (M1, etc.).\n>* The model will require 5GB of free disk space, which you can free up when not in use.\n\n### Step 1. Download and install Ollama\n* Browse to: [https://ollama.com](https://ollama.com/)\n* Click the Download button\n* Or you could just browse to: [https://ollama.com/download](https://ollama.com/download)\n* Click Download for macOS\n\n>On a Mac, (at the time of this writing) this will download a *.zip file to your ~/Downloads folder.\n\nIn Finder double click the *.zip file to extract the contents\nThis should extract Ollama.app to your ~/Downloads folder\nDrag Ollama.app to your Applications folder\nYou can then delete the downloaded zip file to save space\n\n### Step 2. Setup Ollama\n\nAfter you download Ollama you will need to run the setup wizard:\n\nIn Finder, browse to the Applications folder\nDouble-click on Ollama\nWhen you see the warning, click Open\nGo through the setup wizard where it should prompt you to install the command line version (ollama)\nThen it will give you instructions for running a model\nAt the time of this writing, the default instructions show llama2, but llama3 works too\nClick Finish\n\n\n### Step 3. Run llama 3\nYou could follow the instruction to run llama 2, but let's jump right in with llama 3\nOpen a new Terminal window\nRun this command (note that for this command llama3 is one word):\n\n```javascript:Terminal\nollama run llama3\n``` \n\nThe first time you run that for a new model, it will download the latest version\n\n\n### Step 4. Chat with llama 3\n\nAfter the model is downloaded, you should see a prompt like this:\n\n` >>> Send a message (/? for help)`\n\nAt the prompt, start chatting with it like any other LLM\n\n### Additional models\nYou can use other models, besides just llama2 and llama3.\n\nFor more models, click the Models tab on https://ollama.com or visit: https://ollama.com/library\n\n### Code Llama and Llama 3\nHere is what meta.ai says about Code Llama and Llama 3\n\nCode Llama, a separate AI model designed for code understanding and generation, was integrated into LLaMA 3 (Large Language Model Meta AI) to enhance its coding capabilities. This integration enabled LLaMA 3 to leverage Code Llama's expertise in code-related tasks, such as:\n\nCode completion\nCode generation\nCode explanation\nCode review\nThe integration allows LLaMA 3 to tap into Code Llama's knowledge base, which was trained on a massive dataset of code from various sources, including open-source repositories and coding platforms. This enables LLaMA 3 to provide more accurate and informative responses to coding-related queries and tasks. In essence, the integration of Code Llama into LLaMA 3 creates a powerful hybrid AI model that can tackle a wide range of tasks, from general knowledge and conversation to coding and software development.\n\nIf you have any questions please leave them in the comments.",
    "code": "var Component=(()=>{var m=Object.create;var t=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var g=Object.getPrototypeOf,w=Object.prototype.hasOwnProperty;var f=(n,e)=>()=>(e||n((e={exports:{}}).exports,e),e.exports),y=(n,e)=>{for(var l in e)t(n,l,{get:e[l],enumerable:!0})},r=(n,e,l,i)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let o of p(e))!w.call(n,o)&&o!==l&&t(n,o,{get:()=>e[o],enumerable:!(i=u(e,o))||i.enumerable});return n};var b=(n,e,l)=>(l=n!=null?m(g(n)):{},r(e||!n||!n.__esModule?t(l,\"default\",{value:n,enumerable:!0}):l,n)),L=n=>r(t({},\"__esModule\",{value:!0}),n);var s=f((v,d)=>{d.exports=_jsx_runtime});var M={};y(M,{default:()=>h,frontmatter:()=>k});var a=b(s()),k={title:\"Install Ollama and llama3 on MacOS\",date:\"2024-05-24\",tags:[\"Ollama\",\"guide\"],draft:!1,summary:\"Guide on how to install Ollama and Llama3 on MacOS.\"};function c(n){let e={a:\"a\",blockquote:\"blockquote\",code:\"code\",figure:\"figure\",h1:\"h1\",h3:\"h3\",li:\"li\",p:\"p\",pre:\"pre\",span:\"span\",ul:\"ul\",...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.h1,{children:\"Install Ollama on MacOS\"}),`\n`,(0,a.jsx)(e.h3,{children:\"what is Ollama?\"}),`\n`,(0,a.jsx)(e.p,{children:\"Ollama\\xA0is an open-source tool that allows you to run and customize large language models locally on your own machine.\"}),`\n`,(0,a.jsx)(e.p,{children:\"Here are some key points about Ollama:\"}),`\n`,(0,a.jsxs)(e.ul,{children:[`\n`,(0,a.jsx)(e.li,{children:\"Local Integration: Ollama enables you to work with language models directly on your machine, avoiding the need for external services or APIs.\"}),`\n`,(0,a.jsx)(e.li,{children:\"Model Support: It supports various language models, including Llama 3, Mistral, Gemma, and more. You can choose the model that best suits your needs.\"}),`\n`,(0,a.jsx)(e.li,{children:\"CLI and REST API: Ollama provides a command-line interface (CLI) for running and customizing models. Additionally, it offers a REST API for programmatic access.\"}),`\n`,(0,a.jsx)(e.li,{children:\"Customization: You can create and customize models using Ollama. For example, you can adjust parameters like temperature to control creativity and coherence.\"}),`\n`,(0,a.jsx)(e.li,{children:\"Model Library: Ollama includes a library of pre-trained models that you can download and use. These models cover a range of sizes and capabilities.\"}),`\n`,(0,a.jsx)(e.li,{children:\"RAM Requirements: Keep in mind that running larger models (e.g., 7B models) may require a minimum of 8 GB of available RAM.\"}),`\n`]}),`\n`,(0,a.jsx)(e.h3,{children:\"In this article, I will show you how to get started with Ollama on a Mac.\"}),`\n`,(0,a.jsxs)(e.blockquote,{children:[`\n`,(0,a.jsxs)(e.ul,{children:[`\n`,(0,a.jsx)(e.li,{children:\"These instructions were written for and tested on a Mac (M1, 8GB).\"}),`\n`,(0,a.jsx)(e.li,{children:\"You will have much better success on a Mac that uses\\xA0Apple Silicon\\xA0(M1, etc.).\"}),`\n`,(0,a.jsx)(e.li,{children:\"The model will require\\xA05GB\\xA0of free disk space, which you can free up when not in use.\"}),`\n`]}),`\n`]}),`\n`,(0,a.jsx)(e.h3,{children:\"Step 1. Download and install Ollama\"}),`\n`,(0,a.jsxs)(e.ul,{children:[`\n`,(0,a.jsxs)(e.li,{children:[\"Browse to:\\xA0\",(0,a.jsx)(e.a,{href:\"https://ollama.com/\",children:\"https://ollama.com\"})]}),`\n`,(0,a.jsx)(e.li,{children:\"Click the\\xA0Download\\xA0button\"}),`\n`,(0,a.jsxs)(e.li,{children:[\"Or you could just browse to:\\xA0\",(0,a.jsx)(e.a,{href:\"https://ollama.com/download\",children:\"https://ollama.com/download\"})]}),`\n`,(0,a.jsx)(e.li,{children:\"Click\\xA0Download for macOS\"}),`\n`]}),`\n`,(0,a.jsxs)(e.blockquote,{children:[`\n`,(0,a.jsx)(e.p,{children:\"On a Mac, (at the time of this writing) this will download a *.zip file to your ~/Downloads folder.\"}),`\n`]}),`\n`,(0,a.jsx)(e.p,{children:`In Finder double click the *.zip file to extract the contents\nThis should extract Ollama.app to your ~/Downloads folder\nDrag Ollama.app to your Applications folder\nYou can then delete the downloaded zip file to save space`}),`\n`,(0,a.jsx)(e.h3,{children:\"Step 2. Setup Ollama\"}),`\n`,(0,a.jsx)(e.p,{children:\"After you download Ollama you will need to run the setup wizard:\"}),`\n`,(0,a.jsx)(e.p,{children:`In Finder, browse to the Applications folder\nDouble-click on Ollama\nWhen you see the warning, click Open\nGo through the setup wizard where it should prompt you to install the command line version (ollama)\nThen it will give you instructions for running a model\nAt the time of this writing, the default instructions show llama2, but llama3 works too\nClick Finish`}),`\n`,(0,a.jsx)(e.h3,{children:\"Step 3. Run llama 3\"}),`\n`,(0,a.jsx)(e.p,{children:`You could follow the instruction to run llama 2, but let's jump right in with llama 3\nOpen a new Terminal window\nRun this command (note that for this command llama3 is one word):`}),`\n`,(0,a.jsx)(e.figure,{\"data-rehype-pretty-code-figure\":\"\",children:(0,a.jsx)(e.pre,{tabIndex:\"0\",\"data-language\":\"javascript:Terminal\",\"data-theme\":\"github-dark\",children:(0,a.jsx)(e.code,{\"data-language\":\"javascript:Terminal\",\"data-theme\":\"github-dark\",style:{display:\"grid\"},children:(0,a.jsx)(e.span,{\"data-line\":\"\",children:(0,a.jsx)(e.span,{children:\"ollama run llama3\"})})})})}),`\n`,(0,a.jsx)(e.p,{children:\"The first time you run that for a new model, it will download the latest version\"}),`\n`,(0,a.jsx)(e.h3,{children:\"Step 4. Chat with llama 3\"}),`\n`,(0,a.jsx)(e.p,{children:\"After the model is downloaded, you should see a prompt like this:\"}),`\n`,(0,a.jsx)(e.p,{children:(0,a.jsx)(e.code,{children:\" >>> Send a message (/? for help)\"})}),`\n`,(0,a.jsx)(e.p,{children:\"At the prompt, start chatting with it like any other LLM\"}),`\n`,(0,a.jsx)(e.h3,{children:\"Additional models\"}),`\n`,(0,a.jsx)(e.p,{children:\"You can use other models, besides just llama2 and llama3.\"}),`\n`,(0,a.jsxs)(e.p,{children:[\"For more models, click the Models tab on \",(0,a.jsx)(e.a,{href:\"https://ollama.com\",children:\"https://ollama.com\"}),\" or visit: \",(0,a.jsx)(e.a,{href:\"https://ollama.com/library\",children:\"https://ollama.com/library\"})]}),`\n`,(0,a.jsx)(e.h3,{children:\"Code Llama and Llama 3\"}),`\n`,(0,a.jsx)(e.p,{children:\"Here is what meta.ai says about Code Llama and Llama 3\"}),`\n`,(0,a.jsx)(e.p,{children:\"Code Llama, a separate AI model designed for code understanding and generation, was integrated into LLaMA 3 (Large Language Model Meta AI) to enhance its coding capabilities. This integration enabled LLaMA 3 to leverage Code Llama's expertise in code-related tasks, such as:\"}),`\n`,(0,a.jsx)(e.p,{children:`Code completion\nCode generation\nCode explanation\nCode review\nThe integration allows LLaMA 3 to tap into Code Llama's knowledge base, which was trained on a massive dataset of code from various sources, including open-source repositories and coding platforms. This enables LLaMA 3 to provide more accurate and informative responses to coding-related queries and tasks. In essence, the integration of Code Llama into LLaMA 3 creates a powerful hybrid AI model that can tackle a wide range of tasks, from general knowledge and conversation to coding and software development.`}),`\n`,(0,a.jsx)(e.p,{children:\"If you have any questions please leave them in the comments.\"})]})}function h(n={}){let{wrapper:e}=n.components||{};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(c,{...n})}):c(n)}return L(M);})();\n;return Component;"
  },
  "_id": "Install-Ollama-on-MacOS.mdx",
  "_raw": {
    "sourceFilePath": "Install-Ollama-on-MacOS.mdx",
    "sourceFileName": "Install-Ollama-on-MacOS.mdx",
    "sourceFileDir": ".",
    "contentType": "mdx",
    "flattenedPath": "Install-Ollama-on-MacOS"
  },
  "type": "Post",
  "url": "/blog/Install-Ollama-on-MacOS",
  "slug": "Install-Ollama-on-MacOS",
  "readingTime": 5
}